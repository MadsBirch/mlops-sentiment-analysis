{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/madsbirch/Documents/4_semester/mlops/mlops-sentiment-analysis\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/Users/madsbirch/Documents/4_semester/mlops/mlops-sentiment-analysis')\n",
    "print(\"Current working directory: {0}\".format(os.getcwd()))\n",
    "\n",
    "import gzip, json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "\n",
    "\n",
    "from src.data.AmazonReviewData import AmazonReviewsDataset\n",
    "from src.data.make_dataset_temp import get_pandas_DF, sentiment_map, preprocess_data, get_dataloaders\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Set device (mps is specific to mac with M1 processor)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# path to raw data\n",
    "raw_data_path = \"data/raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Data exporation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be downloaded from: http://jmcauley.ucsd.edu/data/amazon/links.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>I needed a set of jumper cables for my new car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>These long cables work fine for my truck, but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Can't comment much on these since they have no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>I absolutley love Amazon!!!  For the price of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>I purchased the 12' feet long cable set and th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          2  I needed a set of jumper cables for my new car...\n",
       "1          2  These long cables work fine for my truck, but ...\n",
       "2          2  Can't comment much on these since they have no...\n",
       "3          2  I absolutley love Amazon!!!  For the price of ...\n",
       "4          2  I purchased the 12' feet long cable set and th..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_pandas_DF(raw_data_path+'reviews_Automotive_5.json.gz')\n",
    "# subset columns and rename to more intuitive names \n",
    "df = df[['overall', 'reviewText']]\n",
    "df = df.rename(columns={'overall': 'sentiment', 'reviewText': 'review'})\n",
    "  \n",
    "# do sentiment mapping\n",
    "df.sentiment = df.sentiment.apply(sentiment_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are highly imbalanced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    17895\n",
       "1     1430\n",
       "0     1148\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_dataloaders(batch_size = 12, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "#model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model\n",
    "\n",
    "for batch in train_loader:\n",
    "    review = batch['review']\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "    \n",
    "    out = model(input_ids, attention_mask)\n",
    "    pooled_out = out['pooler_output']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7009,  0.4387,  0.9998,  ...,  0.9999, -0.7846,  0.9871],\n",
       "        [-0.7625,  0.4366,  0.9999,  ...,  1.0000, -0.7165,  0.9935],\n",
       "        [-0.7900,  0.4732,  0.9999,  ...,  1.0000, -0.7976,  0.9955],\n",
       "        ...,\n",
       "        [-0.7630,  0.4506,  0.9999,  ...,  1.0000, -0.7497,  0.9906],\n",
       "        [-0.6988,  0.2968,  0.9996,  ...,  0.9999, -0.9025,  0.9821],\n",
       "        [-0.6581,  0.3612,  0.9997,  ...,  0.9999, -0.5910,  0.9741]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL INFO\n",
    "\n",
    "A description of the BERT model can be found at: https://huggingface.co/bert-base-uncased\n",
    "\n",
    "We use the uncased model, which does not differ between Mads and mads (capital letters are ignored.)\n",
    "\n",
    "It would be nice exploring how to download smaller pretrained BERT-models as discussed in this articel (https://arxiv.org/abs/1908.08962), as those would be trainable locally. The smaller models mentioned in the article are available at: https://github.com/google-research/bert/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSentiment(nn.Module):\n",
    "  def __init__(self, n_classes: int, dropout: float, bert_out_dim = 768):\n",
    "    super(BertSentiment, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.output = nn.Linear(bert_out_dim, n_classes)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.bert(input_ids, attention_mask)\n",
    "    pooled_output = output['pooler_output']\n",
    "    x = self.dropout(pooled_output)\n",
    "    out = self.output(x)\n",
    "    \n",
    "    return out\n",
    "\n",
    "### NOT USED AND NOT WORKING. IF WE WANT TO USE THE OUTPUT OF THE MODEL NEEDS TO BE POOLED SOMEHOW ###\n",
    "class DistilBertSentiment(nn.Module):\n",
    "  def __init__(self, n_classes: int, dropout: float, bert_out_dim = 3072):\n",
    "    super(DistilBertSentiment, self).__init__()\n",
    "    self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.output = nn.Linear(bert_out_dim, n_classes)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.bert(input_ids, attention_mask)\n",
    "    pooled_output = output['pooler_output']\n",
    "    x = self.dropout(pooled_output)\n",
    "    out = self.output(x)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# get data loader\n",
    "train_loader, test_loader = get_dataloaders(batch_size = 12, num_workers = 0)\n",
    "\n",
    "# init model\n",
    "model = BertSentiment(n_classes=3, dropout=0.2).to(device)\n",
    "\n",
    "# training params\n",
    "num_epochs = 1\n",
    "lr = 1e-4\n",
    "\n",
    "# define criterion and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH]:   1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1195/1195 [26:03<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# BASIC TRAINNIG LOOP\n",
    "\n",
    "for e in range(num_epochs):\n",
    "  print(f'[EPOCH]: {e+1:3d}')\n",
    "  \n",
    "  model.train()\n",
    "  for batch in tqdm(train_loader):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # move data to device\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    # get model outout and calc loss\n",
    "    output = model(input_ids, attention_mask)\n",
    "    loss = loss_fn(output, labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model on single samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REVIEW]: My car: 2004 Honda (Acura) TLI've installed these on the rear blinkers. For blinker applications, load resistors are often required and I've installed them. These bulbs are not much brighter than traditional incandescent bulbs but do look nice. The socket was not an exact match for the 7440 bulbs, so I had to modify the socket and had to solder the terminals directly onto the socket, which I do not recommend unless you're skilled in soldering.\n",
      "[LABEL]: 2\n",
      "[PREDICTED LABEL]: 2\n"
     ]
    }
   ],
   "source": [
    "### TEST MODEL ON SPECIFIC SAMPLES FROM TEST SET ###\n",
    "\n",
    "# pick smaple ID:\n",
    "test_id = 111\n",
    "\n",
    "# define tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "train_df, test_df, _, _ = preprocess_data(raw_data_path, tokenizer)\n",
    "\n",
    "# get test sample\n",
    "test_review = test_df['review'].iloc[test_id]\n",
    "original_label = test_df['sentiment'].iloc[test_id]\n",
    "\n",
    "# encode sample\n",
    "encoding = tokenizer.encode_plus(\n",
    "    test_review,\n",
    "    add_special_tokens=True,\n",
    "    max_length=256,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(f'[REVIEW]: {test_review}')\n",
    "print(f'[LABEL]: {original_label}')\n",
    "\n",
    "# get prediction for sample\n",
    "with torch.no_grad():\n",
    "  model.to('cpu')\n",
    "  \n",
    "  input_ids = encoding['input_ids'].to('cpu')\n",
    "  attention_mask = encoding['attention_mask'].to('cpu')\n",
    "  \n",
    "  preds = model(input_ids, attention_mask)\n",
    "  preds = torch.argmax(F.softmax(preds, dim = 1)).detach().numpy()\n",
    "  print(f'[PREDICTED LABEL]: {preds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model on entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader, display=True):\n",
    "    model.eval().to('cpu')\n",
    "    \n",
    "    test_loss = 0\n",
    "    n_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    TEST_ACC = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            \n",
    "            review = batch['review']\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            preds = torch.argmax(F.softmax(outputs, dim = 1),dim=1)\n",
    "            n_correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = (n_correct/total)*100\n",
    "    \n",
    "    if display:\n",
    "        print(f'Accuracy on the test set: {acc:.2f} %')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [08:04<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 87.82 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfdbeb5e6c2345cb32cc8726be7b62a559fcaacb25273dff681832fd4a016f0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
