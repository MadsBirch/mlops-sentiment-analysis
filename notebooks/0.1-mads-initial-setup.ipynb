{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/madsbirch/Documents/4_semester/mlops/mlops-sentiment-analysis\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/Users/madsbirch/Documents/4_semester/mlops/mlops-sentiment-analysis')\n",
    "print(\"Current working directory: {0}\".format(os.getcwd()))\n",
    "\n",
    "import gzip, json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "\n",
    "\n",
    "from src.data.AmazonReviewData import AmazonReviewsDataset\n",
    "from src.data.make_dataset_temp import get_pandas_DF, sentiment_map, preprocess_data, get_dataloaders\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Set device (mps is specific to mac with M1 processor)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# path to raw data\n",
    "raw_data_path = \"data/raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Data exporation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be downloaded from: http://jmcauley.ucsd.edu/data/amazon/links.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>I needed a set of jumper cables for my new car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>These long cables work fine for my truck, but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Can't comment much on these since they have no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>I absolutley love Amazon!!!  For the price of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>I purchased the 12' feet long cable set and th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          2  I needed a set of jumper cables for my new car...\n",
       "1          2  These long cables work fine for my truck, but ...\n",
       "2          2  Can't comment much on these since they have no...\n",
       "3          2  I absolutley love Amazon!!!  For the price of ...\n",
       "4          2  I purchased the 12' feet long cable set and th..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_pandas_DF(raw_data_path+'reviews_Automotive_5.json.gz')\n",
    "# subset columns and rename to more intuitive names \n",
    "df = df[['overall', 'reviewText']]\n",
    "df = df.rename(columns={'overall': 'sentiment', 'reviewText': 'review'})\n",
    "  \n",
    "# do sentiment mapping\n",
    "df.sentiment = df.sentiment.apply(sentiment_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are highly imbalanced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    17895\n",
       "1     1430\n",
       "0     1148\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_dataloaders(batch_size = 12, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-cased')\n",
    "#model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model\n",
    "\n",
    "for batch in train_loader:\n",
    "    review = batch['review']\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "    \n",
    "    out = model(input_ids, attention_mask)\n",
    "    pooled_out = out['pooler_output']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7009,  0.4387,  0.9998,  ...,  0.9999, -0.7846,  0.9871],\n",
       "        [-0.7625,  0.4366,  0.9999,  ...,  1.0000, -0.7165,  0.9935],\n",
       "        [-0.7900,  0.4732,  0.9999,  ...,  1.0000, -0.7976,  0.9955],\n",
       "        ...,\n",
       "        [-0.7630,  0.4506,  0.9999,  ...,  1.0000, -0.7497,  0.9906],\n",
       "        [-0.6988,  0.2968,  0.9996,  ...,  0.9999, -0.9025,  0.9821],\n",
       "        [-0.6581,  0.3612,  0.9997,  ...,  0.9999, -0.5910,  0.9741]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL INFO\n",
    "\n",
    "A description of the BERT model can be found at: https://huggingface.co/bert-base-uncased\n",
    "\n",
    "We use the uncased model, which does not differ between Mads and mads (capital letters are ignored.)\n",
    "\n",
    "It would be nice exploring how to download smaller pretrained models, as those would be trainable locally. A wide range of smaller models are available at: https://github.com/google-research/bert/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSentiment(nn.Module):\n",
    "  def __init__(self, n_classes: int, dropout: float, bert_out_dim = 768):\n",
    "    super(BertSentiment, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.output = nn.Linear(bert_out_dim, n_classes)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.bert(input_ids, attention_mask)\n",
    "    pooled_output = output['pooler_output']\n",
    "    x = self.dropout(pooled_output)\n",
    "    out = self.output(x)\n",
    "    \n",
    "    return out\n",
    "\n",
    "### NOT USED AND NOT WORKING. IF WE WANT TO USE THE OUTPUT OF THE MODEL NEEDS TO BE POOLED SOMEHOW ###\n",
    "class DistilBertSentiment(nn.Module):\n",
    "  def __init__(self, n_classes: int, dropout: float, bert_out_dim = 3072):\n",
    "    super(DistilBertSentiment, self).__init__()\n",
    "    self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.output = nn.Linear(bert_out_dim, n_classes)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.bert(input_ids, attention_mask)\n",
    "    pooled_output = output['pooler_output']\n",
    "    x = self.dropout(pooled_output)\n",
    "    out = self.output(x)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertSentiment(n_classes=3, dropout=0.2).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH]:   0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 309/1195 [07:14<27:12,  1.84s/it]"
     ]
    }
   ],
   "source": [
    "# BASIC TRAINNIG LOOP\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print(f'[EPOCH]: {epoch:3d}')\n",
    "  \n",
    "  model.train()\n",
    "  for batch in tqdm(train_loader):\n",
    "    \n",
    "    # move data to device\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    # get model outout and calc loss\n",
    "    output = model(input_ids, attention_mask)\n",
    "    loss = criterion(output, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfdbeb5e6c2345cb32cc8726be7b62a559fcaacb25273dff681832fd4a016f0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
